{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6b60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle #for saving/loading trained classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a8d7b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding our files - make sure to follow the same file/folder structure.\n",
    "file_data = '.' + os.sep + 'data' + os.sep +'metadata.csv'\n",
    "path_image = '.' + os.sep + 'data' + os.sep + 'images' + os.sep + 'imgs_part_1'\n",
    "\n",
    "# read the metadata csv and find the diagnostic labels.\n",
    "df = pd.read_csv(file_data)\n",
    "\n",
    "# Find the features from the feature extraction.\n",
    "file_features = 'features/features.csv'\n",
    "feature_names = ['file_name','asymmetry','color','blue-white_veil']\n",
    "\n",
    "# Load up the features in a separate dataframe to filter our metadata.\n",
    "df_features = pd.read_csv(file_features)\n",
    "\n",
    "# our_list for all images in the metadata that we also have in our features.csv\n",
    "our_list = list(np.array(df_features[\"file_name\"]))\n",
    "filtered_data = df[df[\"img_id\"].isin(our_list)]\n",
    "label = np.array(filtered_data['diagnostic'])\n",
    "image_id = list(filtered_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1607c361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the dataset\n",
    "x = np.array(df_features[feature_names[1:]])\n",
    "\n",
    "y = np.zeros(len(label))  # Initialize the labels array with zeros\n",
    "y[label == 'BCC'] = 1     # Set BCC samples to 1\n",
    "y[label == 'SCC'] = 2     # Set SCC samples to 2\n",
    "y[label == 'MEL'] = 3     # Set MEL samples to 3\n",
    "patient_id = filtered_data['patient_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bed24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in the first 80% train data and 20% true test data for the final validation.\n",
    "\n",
    "x_train_data, x_test_data, y_train_data, y_test_data, patient_id_train_data, patient_id_test_data = train_test_split(\n",
    "    x, y, patient_id, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb22210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare cross-validation -\n",
    "# GroupKFold makes sure patients with the same ID will not be split between the training and validation sets.\n",
    "\n",
    "num_folds = 5\n",
    "group_kfold = GroupKFold(n_splits=num_folds)\n",
    "group_kfold.get_n_splits(x_train_data, y_train_data, patient_id_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classifiers are defined here. We use K-NN\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(1),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(5),\n",
    "    KNeighborsClassifier(7),\n",
    "    KNeighborsClassifier(9),\n",
    "    LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "    RandomForestClassifier(100,max_depth=1,random_state=1907),\n",
    "    DecisionTreeClassifier(random_state=1907)\n",
    "]\n",
    "num_classifiers = len(classifiers)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d227ae31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up np arrays for the eventual accuracy- and F1-scores.\n",
    "acc_val = np.empty([num_folds,num_classifiers])\n",
    "f1_val = np.empty([num_folds, num_classifiers])\n",
    "roc_auc_val = np.empty([num_folds, num_classifiers])\n",
    "specificity_val = np.empty([num_folds, num_classifiers])\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "specificity_list = []\n",
    "sensitivity_list = []\n",
    "\n",
    "# Splits up our data into training and validation sets at a 80/20 ratio. The group_kfold does training across folds,\n",
    "# with a default of 5 folds it will give us 5 outputs.\n",
    "for i, (train_index, val_index) in enumerate(group_kfold.split(x_train_data, y_train_data, patient_id_train_data)):\n",
    "    \n",
    "    # x_train = 80%\n",
    "    # y_train = truth for 80%\n",
    "    # x_val = 20%\n",
    "    # y_val = truth for 20%\n",
    "    x_train = x[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    x_val = x[val_index,:]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    " \n",
    "    # Standardize features\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    \n",
    "    \n",
    "    acc_l = []\n",
    "    f1_l = []\n",
    "    roc_auc_l = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    \n",
    "    for j, clf in enumerate(classifiers): \n",
    "        # Train the classifier with the 80%.\n",
    "        clf.fit(x_train_scaled, y_train)\n",
    "        \n",
    "        # Predict labels for validation data\n",
    "        y_pred = clf.predict(x_val_scaled)\n",
    "    \n",
    "        # Evaluate accuracy score (mostly useless)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        acc_val[i, j] = accuracy\n",
    "        acc_l.append(accuracy)\n",
    "        \n",
    "        # Evaluate F1 score (Shows us the ratio of false positives and true negatives)\n",
    "        multi_cm = multilabel_confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        f1_val[i, j] = f1\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        \n",
    "        # Calculate ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict_proba(x_val_scaled), multi_class='ovr')\n",
    "        roc_auc_val[i, j] = roc_auc\n",
    "        roc_auc_l.append(roc_auc)\n",
    "        \n",
    "        # Calculate specificity\n",
    "        c1, c2, c3, c4 = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        tn = c1[0] # true negative\n",
    "        tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "        fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "        fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "        specificity.append(tn/(tn+fp))\n",
    "        \n",
    "        sensitivity.append(tp/(tp+fn))\n",
    "        \n",
    "        \n",
    "        # Print our scores - Classifier 1 is K-NN(1) and Classifier 2 is K-NN(5):\n",
    "        #print(f\"F1 score (Fold {i + 1}, Classifier {j + 1}): {f1_l}\")\n",
    "        #print(f\"ROC AUC score (Fold {i + 1}, Classifier {j + 1}): {roc_auc_l}\")\n",
    "        #print(f\"Specificity (Fold {i + 1}, Classifier {j + 1}): {specificity}\")\n",
    "        #print(f\"Sensitivity (Fold {i + 1}, Classifier {j + 1}): {sensitivity}\")\n",
    "        #print(\"\")\n",
    "        if len(acc_l) == len(classifiers):\n",
    "            acc_list.append(acc_l)\n",
    "        if len(f1_l) == len(classifiers):\n",
    "            f1_list.append(f1_l)\n",
    "        if len(roc_auc_l) == len(classifiers):\n",
    "            roc_auc_list.append(roc_auc_l)\n",
    "        if len(specificity) == len(classifiers):\n",
    "            specificity_list.append(specificity)\n",
    "        if len(sensitivity) == len(classifiers):\n",
    "            sensitivity_list.append(sensitivity)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cde012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate means and standard deviations for each classifier across all folds\n",
    "means = {}\n",
    "std_devs = {}\n",
    "\n",
    "for classifier_idx in range(len(classifiers)):\n",
    "    classifier_name = f\"Classifier {classifier_idx+1}\"\n",
    "    \n",
    "    # Extract scores for the current classifier across all folds\n",
    "    classifier_f1_scores = [fold[classifier_idx] for fold in f1_list]\n",
    "    classifier_accuracy_scores = [fold[classifier_idx] for fold in acc_list]\n",
    "    classifier_sensitivity_scores = [fold[classifier_idx] for fold in sensitivity_list]\n",
    "    classifier_specificity_scores = [fold[classifier_idx] for fold in specificity_list]\n",
    "    classifier_roc_auc_scores = [fold[classifier_idx] for fold in roc_auc_list]\n",
    "    \n",
    "    # Convert scores to NumPy arrays for easier calculations\n",
    "    classifier_f1_scores_array = np.array(classifier_f1_scores)\n",
    "    classifier_accuracy_scores_array = np.array(classifier_accuracy_scores)\n",
    "    classifier_sensitivity_scores_array = np.array(classifier_sensitivity_scores)\n",
    "    classifier_specificity_scores_array = np.array(classifier_specificity_scores)\n",
    "    classifier_roc_auc_scores_array = np.array(classifier_roc_auc_scores)\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    means[classifier_name] = {\n",
    "        'F1 Score': np.mean(classifier_f1_scores_array),\n",
    "        'Accuracy Score': np.mean(classifier_accuracy_scores_array),\n",
    "        'Sensitivity Score': np.mean(classifier_sensitivity_scores_array),\n",
    "        'Specificity Score': np.mean(classifier_specificity_scores_array),\n",
    "        'ROC AUC Score': np.mean(classifier_roc_auc_scores_array)\n",
    "    }\n",
    "\n",
    "    std_devs[classifier_name] = {\n",
    "        'F1 Score': np.std(classifier_f1_scores_array),\n",
    "        'Accuracy Score': np.std(classifier_accuracy_scores_array),\n",
    "        'Sensitivity Score': np.std(classifier_sensitivity_scores_array),\n",
    "        'Specificity Score': np.std(classifier_specificity_scores_array),\n",
    "        'ROC AUC Score': np.std(classifier_roc_auc_scores_array)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10071c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores for each classifier:\n",
      "              F1 Score  Accuracy Score  Sensitivity Score  Specificity Score  \\\n",
      "Classifier 1  0.449899        0.451385           0.399305           0.487960   \n",
      "Classifier 2  0.427916        0.467590           0.258530           0.637645   \n",
      "Classifier 3  0.455079        0.501190           0.278377           0.690550   \n",
      "Classifier 4  0.429104        0.478015           0.240972           0.675365   \n",
      "Classifier 5  0.437378        0.488446           0.239127           0.711086   \n",
      "Classifier 6  0.404596        0.472207           0.156883           0.762489   \n",
      "Classifier 7  0.398366        0.514982           0.108109           0.911650   \n",
      "Classifier 8  0.383640        0.385428           0.354055           0.405009   \n",
      "\n",
      "              ROC AUC Score  \n",
      "Classifier 1       0.522897  \n",
      "Classifier 2       0.545870  \n",
      "Classifier 3       0.562815  \n",
      "Classifier 4       0.546674  \n",
      "Classifier 5       0.559348  \n",
      "Classifier 6       0.569223  \n",
      "Classifier 7       0.601285  \n",
      "Classifier 8       0.486264  \n",
      "\n",
      "Standard deviation score sfor each classifier:\n",
      "              F1 Score  Accuracy Score  Sensitivity Score  Specificity Score  \\\n",
      "Classifier 1  0.029396        0.024965           0.064048           0.018266   \n",
      "Classifier 2  0.022667        0.018321           0.048640           0.030863   \n",
      "Classifier 3  0.023028        0.023984           0.046460           0.050062   \n",
      "Classifier 4  0.018923        0.014105           0.068813           0.034947   \n",
      "Classifier 5  0.026803        0.027846           0.054303           0.062932   \n",
      "Classifier 6  0.028767        0.023263           0.039913           0.050142   \n",
      "Classifier 7  0.081957        0.039117           0.113657           0.088137   \n",
      "Classifier 8  0.014019        0.017615           0.052482           0.029430   \n",
      "\n",
      "              ROC AUC Score  \n",
      "Classifier 1       0.010465  \n",
      "Classifier 2       0.055054  \n",
      "Classifier 3       0.052467  \n",
      "Classifier 4       0.050379  \n",
      "Classifier 5       0.052115  \n",
      "Classifier 6       0.064911  \n",
      "Classifier 7       0.049201  \n",
      "Classifier 8       0.009040  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the means dictionary to a DataFrame\n",
    "means_df = pd.DataFrame.from_dict(means, orient='index')\n",
    "\n",
    "std_df = pd.DataFrame.from_dict(std_devs, orient='index')\n",
    "\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Mean scores for each classifier:\")\n",
    "print(means_df)\n",
    "print()\n",
    "\n",
    "print(\"Standard deviation score sfor each classifier:\")\n",
    "print(std_df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676743dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define final classifier:\n",
    "classifier = KNeighborsClassifier(5)\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_data)\n",
    "\n",
    "# Transform both the training and testing data using the fitted scaler\n",
    "x_train_data_scaled = scaler.transform(x_train_data)\n",
    "x_test_data_scaled = scaler.transform(x_test_data)\n",
    "\n",
    "# Fit the classifier on the scaled training data\n",
    "classifier.fit(x_train_data_scaled, y_train_data)\n",
    "y_train_pred = classifier.predict(x_train_data_scaled)\n",
    "y_test_pred = classifier.predict(x_test_data_scaled)\n",
    "\n",
    "# Calculate training and testing scores\n",
    "train_score = classifier.score(x_train_data_scaled, y_train_data)\n",
    "test_score = classifier.score(x_test_data_scaled, y_test_data)\n",
    "\n",
    "# Evaluation scores, from earlier!\n",
    "train_f1_score = f1_score(y_train_data, y_train_pred, average='weighted')\n",
    "test_f1_score = f1_score(y_test_data, y_test_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        \n",
    "# Calculate ROC AUC score\n",
    "train_roc_auc = roc_auc_score(y_train_data, clf.predict_proba(x_train_data_scaled), multi_class='ovr')\n",
    "test_roc_auc = roc_auc_score(y_test_data, clf.predict_proba(x_test_data_scaled), multi_class='ovr')\n",
    "        \n",
    "# Calculate specificity (first for train)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_train_data, y_train_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "train_specificity = (tn/(tn+fp))\n",
    "        \n",
    "train_sensitivity = (tp/(tp+fn))\n",
    "\n",
    "# Calculate specificity (now for test)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_test_data, y_test_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "test_specificity = (tn/(tn+fp))\n",
    "test_sensitivity = (tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b84b009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for train data: 0.6435185185185185\n",
      "Accuracy score for validation data: 0.5299539170506913\n",
      "Train F1 score: 0.5998769825708896\n",
      "Test F1 score: 0.4873802616313825\n",
      "Train ROC-AUC score: 0.5791117261878536\n",
      "Test ROC-AUC score: 0.5780310476181543\n",
      "Train specificity: 0.8123667377398721\n",
      "Test specificity: 0.7155172413793104\n",
      "Train sensitivity: 0.4430379746835443\n",
      "Test sensitivity: 0.31683168316831684\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score for train data: {train_score}\")\n",
    "print(f\"Accuracy score for validation data: {test_score}\")\n",
    "print(f\"Train F1 score: {train_f1_score}\")\n",
    "print(f\"Test F1 score: {test_f1_score}\")\n",
    "print(f\"Train ROC-AUC score: {train_roc_auc}\")\n",
    "print(f\"Test ROC-AUC score: {test_roc_auc}\")\n",
    "print(f\"Train specificity: {train_specificity}\")\n",
    "print(f\"Test specificity: {test_specificity}\")\n",
    "print(f\"Train sensitivity: {train_sensitivity}\")\n",
    "print(f\"Test sensitivity: {test_sensitivity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467e66d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train our model on all of x and y now that we have validated our results.\n",
    "classifier = classifier.fit(x,y)\n",
    "\n",
    "#This is the classifier you need to save using pickle, add this to your zip file submission\n",
    "filename = 'groupXY_classifier.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
