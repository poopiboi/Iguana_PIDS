{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7a6b60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Default packages for the minimum example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle #for saving/loading trained classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "83a8d7b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding our files - make sure to follow the same file/folder structure.\n",
    "file_data = '.' + os.sep + 'data' + os.sep +'metadata.csv'\n",
    "path_image = '.' + os.sep + 'data' + os.sep + 'images' + os.sep + 'imgs_part_1'\n",
    "\n",
    "# read the metadata csv and find the diagnostic labels.\n",
    "df = pd.read_csv(file_data)\n",
    "\n",
    "# Find the features from the feature extraction.\n",
    "file_features = 'features/features.csv'\n",
    "feature_names = ['file_name','asymmetry','color','blue-white_veil']\n",
    "\n",
    "# Load up the features in a separate dataframe to filter our metadata.\n",
    "df_features = pd.read_csv(file_features)\n",
    "\n",
    "# our_list for all images in the metadata that we also have in our features.csv\n",
    "our_list = list(np.array(df_features[\"file_name\"]))\n",
    "filtered_data = df[df[\"img_id\"].isin(our_list)]\n",
    "label = np.array(filtered_data['diagnostic'])\n",
    "image_id = list(filtered_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1607c361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the dataset\n",
    "x = np.array(df_features[feature_names[1:]])\n",
    "\n",
    "y = np.zeros(len(label))  # Initialize the labels array with zeros\n",
    "y[label == 'BCC'] = 1     # Set BCC samples to 1\n",
    "y[label == 'SCC'] = 2     # Set SCC samples to 2\n",
    "y[label == 'MEL'] = 3     # Set MEL samples to 3\n",
    "patient_id = filtered_data['patient_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2bed24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in the first 80% train data and 20% true test data for the final validation.\n",
    "\n",
    "x_train_data, x_test_data, y_train_data, y_test_data, patient_id_train_data, patient_id_test_data = train_test_split(\n",
    "    x, y, patient_id, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ddb22210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare cross-validation -\n",
    "# GroupKFold makes sure patients with the same ID will not be split between the training and validation sets.\n",
    "\n",
    "num_folds = 5\n",
    "group_kfold = GroupKFold(n_splits=num_folds)\n",
    "group_kfold.get_n_splits(x_train_data, y_train_data, patient_id_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "991a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classifiers are defined here. We use K-NN\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(1),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(5),\n",
    "    KNeighborsClassifier(7),\n",
    "    KNeighborsClassifier(9),\n",
    "    LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "    RandomForestClassifier(100,max_depth=1,random_state=1907),\n",
    "    DecisionTreeClassifier(random_state=1907)\n",
    "]\n",
    "num_classifiers = len(classifiers)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "d227ae31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up np arrays for the eventual accuracy- and F1-scores.\n",
    "acc_val = np.empty([num_folds,num_classifiers])\n",
    "f1_val = np.empty([num_folds, num_classifiers])\n",
    "roc_auc_val = np.empty([num_folds, num_classifiers])\n",
    "specificity_val = np.empty([num_folds, num_classifiers])\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "specificity_list = []\n",
    "sensitivity_list = []\n",
    "\n",
    "# Splits up our data into training and validation sets at a 80/20 ratio. The group_kfold does training across folds,\n",
    "# with a default of 5 folds it will give us 5 outputs.\n",
    "for i, (train_index, val_index) in enumerate(group_kfold.split(x_train_data, y_train_data, patient_id_train_data)):\n",
    "    \n",
    "    # x_train = 80%\n",
    "    # y_train = truth for 80%\n",
    "    # x_val = 20%\n",
    "    # y_val = truth for 20%\n",
    "    x_train = x[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    x_val = x[val_index,:]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    " \n",
    "    # Standardize features\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    \n",
    "    \n",
    "    acc_l = []\n",
    "    f1_l = []\n",
    "    roc_auc_l = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    \n",
    "    for j, clf in enumerate(classifiers): \n",
    "        # Train the classifier with the 80%.\n",
    "        clf.fit(x_train_scaled, y_train)\n",
    "        \n",
    "        # Predict labels for validation data\n",
    "        y_pred = clf.predict(x_val_scaled)\n",
    "    \n",
    "        # Evaluate accuracy score (mostly useless)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        acc_val[i, j] = accuracy\n",
    "        acc_l.append(accuracy)\n",
    "        \n",
    "        # Evaluate F1 score (Shows us the ratio of false positives and true negatives)\n",
    "        multi_cm = multilabel_confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        f1_val[i, j] = f1\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        \n",
    "        # Calculate ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict_proba(x_val_scaled), multi_class='ovr')\n",
    "        roc_auc_val[i, j] = roc_auc\n",
    "        roc_auc_l.append(roc_auc)\n",
    "        \n",
    "        # Calculate specificity\n",
    "        c1, c2, c3, c4 = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        tn = c1[0] # true negative\n",
    "        tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "        fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "        fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "        specificity.append(tn/(tn+fp))\n",
    "        \n",
    "        sensitivity.append(tp/(tp+fn))\n",
    "        \n",
    "        \n",
    "        # Print our scores - Classifier 1 is K-NN(1) and Classifier 2 is K-NN(5):\n",
    "        #print(f\"F1 score (Fold {i + 1}, Classifier {j + 1}): {f1_l}\")\n",
    "        #print(f\"ROC AUC score (Fold {i + 1}, Classifier {j + 1}): {roc_auc_l}\")\n",
    "        #print(f\"Specificity (Fold {i + 1}, Classifier {j + 1}): {specificity}\")\n",
    "        #print(f\"Sensitivity (Fold {i + 1}, Classifier {j + 1}): {sensitivity}\")\n",
    "        #print(\"\")\n",
    "        if len(acc_l) == len(classifiers):\n",
    "            acc_list.append(acc_l)\n",
    "        if len(f1_l) == len(classifiers):\n",
    "            f1_list.append(f1_l)\n",
    "        if len(roc_auc_l) == len(classifiers):\n",
    "            roc_auc_list.append(roc_auc_l)\n",
    "        if len(specificity) == len(classifiers):\n",
    "            specificity_list.append(specificity)\n",
    "        if len(sensitivity) == len(classifiers):\n",
    "            sensitivity_list.append(sensitivity)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3d6a1c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores for Classifier 1: {'F1 Score': 0.44989905628770666, 'Accuracy Score': 0.4513845947035892, 'Sensitivity Score': 0.39930469289164944, 'Specificity Score': 0.4879598662207358, 'ROC AUC Score': 0.5228965546107268}\n",
      "Mean scores for Classifier 2: {'F1 Score': 0.4279156375908385, 'Accuracy Score': 0.46758972980239283, 'Sensitivity Score': 0.25853023909985934, 'Specificity Score': 0.6376446035832415, 'ROC AUC Score': 0.545869551242795}\n",
      "Mean scores for Classifier 3: {'F1 Score': 0.4550785879785943, 'Accuracy Score': 0.5011896760317247, 'Sensitivity Score': 0.27837719074480777, 'Specificity Score': 0.6905503926729718, 'ROC AUC Score': 0.5628148902324653}\n",
      "Mean scores for Classifier 4: {'F1 Score': 0.4291037607823343, 'Accuracy Score': 0.47801451808038714, 'Sensitivity Score': 0.2409718482252142, 'Specificity Score': 0.6753651699270282, 'ROC AUC Score': 0.5466740485676881}\n",
      "Mean scores for Classifier 5: {'F1 Score': 0.43737805479650804, 'Accuracy Score': 0.488446027691894, 'Sensitivity Score': 0.23912690684842586, 'Specificity Score': 0.7110862262038073, 'ROC AUC Score': 0.5593475236793255}\n",
      "Mean scores for Classifier 6: {'F1 Score': 0.40459621696644527, 'Accuracy Score': 0.47220728592552763, 'Sensitivity Score': 0.15688323064926188, 'Specificity Score': 0.7624890578882663, 'ROC AUC Score': 0.5692231768271839}\n",
      "Mean scores for Classifier 7: {'F1 Score': 0.39836621948365164, 'Accuracy Score': 0.514981852399516, 'Sensitivity Score': 0.10810934584075929, 'Specificity Score': 0.9116498778998778, 'ROC AUC Score': 0.6012852925349778}\n",
      "Mean scores for Classifier 8: {'F1 Score': 0.3836399886716707, 'Accuracy Score': 0.3854281489447507, 'Sensitivity Score': 0.3540549571507194, 'Specificity Score': 0.40500899914168575, 'ROC AUC Score': 0.48626391561171933}\n",
      "Standard deviations for Classifier 1: {'F1 Score': 0.029396320534462932, 'Accuracy Score': 0.02496480613710542, 'Sensitivity Score': 0.06404837643611613, 'Specificity Score': 0.018265754011313695, 'ROC AUC Score': 0.010465157697379894}\n",
      "Standard deviations for Classifier 2: {'F1 Score': 0.022666713854945602, 'Accuracy Score': 0.01832085105337841, 'Sensitivity Score': 0.04864045110962722, 'Specificity Score': 0.030862889402944176, 'ROC AUC Score': 0.05505397639634407}\n",
      "Standard deviations for Classifier 3: {'F1 Score': 0.02302843503059811, 'Accuracy Score': 0.023983876689646178, 'Sensitivity Score': 0.046459925129349294, 'Specificity Score': 0.0500619538354511, 'ROC AUC Score': 0.05246701512974598}\n",
      "Standard deviations for Classifier 4: {'F1 Score': 0.01892321164352167, 'Accuracy Score': 0.014105073495721668, 'Sensitivity Score': 0.06881281553883065, 'Specificity Score': 0.03494701444224336, 'ROC AUC Score': 0.05037901705156148}\n",
      "Standard deviations for Classifier 5: {'F1 Score': 0.0268030104619156, 'Accuracy Score': 0.0278460264291906, 'Sensitivity Score': 0.05430269547906562, 'Specificity Score': 0.06293225679529924, 'ROC AUC Score': 0.05211545811733452}\n",
      "Standard deviations for Classifier 6: {'F1 Score': 0.028766858040026027, 'Accuracy Score': 0.023262631416825115, 'Sensitivity Score': 0.03991321166407399, 'Specificity Score': 0.05014234838199, 'ROC AUC Score': 0.06491094280533058}\n",
      "Standard deviations for Classifier 7: {'F1 Score': 0.08195699269991821, 'Accuracy Score': 0.03911742423023571, 'Sensitivity Score': 0.11365687180939575, 'Specificity Score': 0.08813655084867587, 'ROC AUC Score': 0.049200588956956545}\n",
      "Standard deviations for Classifier 8: {'F1 Score': 0.014019081655871833, 'Accuracy Score': 0.017614798154227663, 'Sensitivity Score': 0.05248228782251422, 'Specificity Score': 0.029430382018031904, 'ROC AUC Score': 0.009039863923526365}\n"
     ]
    }
   ],
   "source": [
    "# Calculate means and standard deviations for each classifier across all folds\n",
    "means = {}\n",
    "std_devs = {}\n",
    "\n",
    "for classifier_idx in range(len(classifiers)):\n",
    "    classifier_name = f\"Classifier {classifier_idx+1}\"\n",
    "    \n",
    "    # Extract scores for the current classifier across all folds\n",
    "    classifier_f1_scores = [fold[classifier_idx] for fold in f1_list]\n",
    "    classifier_accuracy_scores = [fold[classifier_idx] for fold in acc_list]\n",
    "    classifier_sensitivity_scores = [fold[classifier_idx] for fold in sensitivity_list]\n",
    "    classifier_specificity_scores = [fold[classifier_idx] for fold in specificity_list]\n",
    "    classifier_roc_auc_scores = [fold[classifier_idx] for fold in roc_auc_list]\n",
    "    \n",
    "    # Convert scores to NumPy arrays for easier calculations\n",
    "    classifier_f1_scores_array = np.array(classifier_f1_scores)\n",
    "    classifier_accuracy_scores_array = np.array(classifier_accuracy_scores)\n",
    "    classifier_sensitivity_scores_array = np.array(classifier_sensitivity_scores)\n",
    "    classifier_specificity_scores_array = np.array(classifier_specificity_scores)\n",
    "    classifier_roc_auc_scores_array = np.array(classifier_roc_auc_scores)\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    means[classifier_name] = {\n",
    "        'F1 Score': np.mean(classifier_f1_scores_array),\n",
    "        'Accuracy Score': np.mean(classifier_accuracy_scores_array),\n",
    "        'Sensitivity Score': np.mean(classifier_sensitivity_scores_array),\n",
    "        'Specificity Score': np.mean(classifier_specificity_scores_array),\n",
    "        'ROC AUC Score': np.mean(classifier_roc_auc_scores_array)\n",
    "    }\n",
    "\n",
    "    std_devs[classifier_name] = {\n",
    "        'F1 Score': np.std(classifier_f1_scores_array),\n",
    "        'Accuracy Score': np.std(classifier_accuracy_scores_array),\n",
    "        'Sensitivity Score': np.std(classifier_sensitivity_scores_array),\n",
    "        'Specificity Score': np.std(classifier_specificity_scores_array),\n",
    "        'ROC AUC Score': np.std(classifier_roc_auc_scores_array)\n",
    "    }\n",
    "\n",
    "# Print or use the means and standard deviations as needed\n",
    "for classifier_name, scores in means.items():\n",
    "    print(f\"Mean scores for {classifier_name}: {scores}\")\n",
    "\n",
    "for classifier_name, scores in std_devs.items():\n",
    "    print(f\"Standard deviations for {classifier_name}: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2b9c7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores for each classifier:\n",
      "              F1 Score  Accuracy Score  Sensitivity Score  Specificity Score  \\\n",
      "Classifier 1  0.449899        0.451385           0.399305           0.487960   \n",
      "Classifier 2  0.427916        0.467590           0.258530           0.637645   \n",
      "Classifier 3  0.455079        0.501190           0.278377           0.690550   \n",
      "Classifier 4  0.429104        0.478015           0.240972           0.675365   \n",
      "Classifier 5  0.437378        0.488446           0.239127           0.711086   \n",
      "Classifier 6  0.404596        0.472207           0.156883           0.762489   \n",
      "Classifier 7  0.398366        0.514982           0.108109           0.911650   \n",
      "Classifier 8  0.383640        0.385428           0.354055           0.405009   \n",
      "\n",
      "              ROC AUC Score  \n",
      "Classifier 1       0.522897  \n",
      "Classifier 2       0.545870  \n",
      "Classifier 3       0.562815  \n",
      "Classifier 4       0.546674  \n",
      "Classifier 5       0.559348  \n",
      "Classifier 6       0.569223  \n",
      "Classifier 7       0.601285  \n",
      "Classifier 8       0.486264  \n"
     ]
    }
   ],
   "source": [
    "# Convert the means dictionary to a DataFrame\n",
    "means_df = pd.DataFrame.from_dict(means, orient='index')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Mean scores for each classifier:\")\n",
    "print(means_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cab0a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define classifers:\n",
    "\n",
    "#train_score = {}\n",
    "#test_score = {}\n",
    "classifier = KNeighborsClassifier(5)\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_data)\n",
    "\n",
    "# Transform both the training and testing data using the fitted scaler\n",
    "x_train_data_scaled = scaler.transform(x_train_data)\n",
    "x_test_data_scaled = scaler.transform(x_test_data)\n",
    "\n",
    "# Fit the classifier on the scaled training data\n",
    "classifier.fit(x_train_data_scaled, y_train_data)\n",
    "\n",
    "\n",
    "y_train_pred = classifier.predict(x_train_data_scaled)\n",
    "y_test_pred = classifier.predict(x_test_data_scaled)\n",
    "\n",
    "# Calculate training and testing scores\n",
    "train_score = classifier.score(x_train_data_scaled, y_train_data)\n",
    "test_score = classifier.score(x_test_data_scaled, y_test_data)\n",
    "\n",
    "\n",
    "# Evaluation scores, from earlier!\n",
    "train_f1_score = f1_score(y_train_data, y_train_pred, average='weighted')\n",
    "test_f1_score = f1_score(y_test_data, y_test_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        \n",
    "# Calculate ROC AUC score\n",
    "train_roc_auc = roc_auc_score(y_train_data, clf.predict_proba(x_train_data_scaled), multi_class='ovr')\n",
    "test_roc_auc = roc_auc_score(y_test_data, clf.predict_proba(x_test_data_scaled), multi_class='ovr')\n",
    "        \n",
    "# Calculate specificity (first for train)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_train_data, y_train_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "train_specificity = (tn/(tn+fp))\n",
    "        \n",
    "train_sensitivity = (tp/(tp+fn))\n",
    "\n",
    "# Calculate specificity (now for test)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_test_data, y_test_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "test_specificity = (tn/(tn+fp))\n",
    "        \n",
    "test_sensitivity = (tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "abda3de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for train data: 0.6435185185185185\n",
      "Accuracy score for validation data: 0.5299539170506913\n",
      "Train F1 score: 0.5998769825708896\n",
      "Test F1 score: 0.4873802616313825\n",
      "Train ROC-AUC score: 0.5791117261878536\n",
      "Test ROC-AUC score: 0.5780310476181543\n",
      "Train specificity: 0.8123667377398721\n",
      "Test specificity: 0.7155172413793104\n",
      "Train sensitivity: 0.4430379746835443\n",
      "Test sensitivity: 0.31683168316831684\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score for train data: {train_score}\")\n",
    "print(f\"Accuracy score for validation data: {test_score}\")\n",
    "print(f\"Train F1 score: {train_f1_score}\")\n",
    "print(f\"Test F1 score: {test_f1_score}\")\n",
    "print(f\"Train ROC-AUC score: {train_roc_auc}\")\n",
    "print(f\"Test ROC-AUC score: {test_roc_auc}\")\n",
    "print(f\"Train specificity: {train_specificity}\")\n",
    "print(f\"Test specificity: {test_specificity}\")\n",
    "print(f\"Train sensitivity: {train_sensitivity}\")\n",
    "print(f\"Test sensitivity: {test_sensitivity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "467e66d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train our model on all of x and y now that we have validated our results.\n",
    "classifier = classifier.fit(x,y)\n",
    "\n",
    "#This is the classifier you need to save using pickle, add this to your zip file submission\n",
    "filename = 'groupXY_classifier.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
