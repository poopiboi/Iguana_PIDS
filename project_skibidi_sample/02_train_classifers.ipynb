{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a6b60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Default packages for the minimum example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle #for saving/loading trained classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83a8d7b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding our files - make sure to follow the same file/folder structure.\n",
    "file_data = '.' + os.sep + 'data' + os.sep +'metadata.csv'\n",
    "path_image = '.' + os.sep + 'data' + os.sep + 'images' + os.sep + 'imgs_part_1'\n",
    "\n",
    "# read the metadata csv and find the diagnostic labels.\n",
    "df = pd.read_csv(file_data)\n",
    "\n",
    "# Find the features from the feature extraction.\n",
    "file_features = 'features/features.csv'\n",
    "feature_names = ['file_name','asymmetry','color','blue-white_veil']\n",
    "\n",
    "# Load up the features in a separate dataframe to filter our metadata.\n",
    "df_features = pd.read_csv(file_features)\n",
    "\n",
    "# our_list for all images in the metadata that we also have in our features.csv\n",
    "our_list = list(np.array(df_features[\"file_name\"]))\n",
    "filtered_data = df[df[\"img_id\"].isin(our_list)]\n",
    "label = np.array(filtered_data['diagnostic'])\n",
    "image_id = list(filtered_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1607c361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# IGNORE - old Y\\ny =  np.int_(filtered_label == 'MEL', 'BCC', 'SCC')   #now True means healthy nevus, False means something else\\n\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the dataset\n",
    "x = np.array(df_features[feature_names[1:]])\n",
    "\n",
    "y = np.zeros(len(label))  # Initialize the labels array with zeros\n",
    "y[label == 'BCC'] = 1     # Set BCC samples to 1\n",
    "y[label == 'SCC'] = 2     # Set SCC samples to 2\n",
    "y[label == 'MEL'] = 3     # Set MEL samples to 3\n",
    "patient_id = filtered_data['patient_id']\n",
<<<<<<< HEAD
    "\n"
=======
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# IGNORE - old Y\n",
    "y =  np.int_(filtered_label == 'MEL', 'BCC', 'SCC')   #now True means healthy nevus, False means something else\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42cde72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       PAT_1516\n",
       "1         PAT_46\n",
       "2       PAT_1545\n",
       "3       PAT_1989\n",
       "4        PAT_684\n",
       "          ...   \n",
       "1178     PAT_931\n",
       "1179     PAT_678\n",
       "1180     PAT_810\n",
       "1181      PAT_83\n",
       "1182    PAT_1220\n",
       "Name: patient_id, Length: 1081, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id"
>>>>>>> aa2d0e0e670ab036551fde860ce92859e7e86883
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bed24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in the first 80% train data and 20% true test data for the final validation.\n",
    "\n",
    "x_train_data, x_test_data, y_train_data, y_test_data, patient_id_train_data, patient_id_test_data = train_test_split(\n",
    "    x, y, patient_id, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb22210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare cross-validation -\n",
    "# GroupKFold makes sure patients with the same ID will not be split between the training and validation sets.\n",
    "\n",
    "num_folds = 5\n",
    "group_kfold = GroupKFold(n_splits=num_folds)\n",
    "group_kfold.get_n_splits(x_train_data, y_train_data, patient_id_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "991a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classifiers are defined here. We use K-NN\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(1),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    KNeighborsClassifier(7),\n",
    "    KNeighborsClassifier(9)\n",
    "]\n",
    "num_classifiers = len(classifiers)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d227ae31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up np arrays for the eventual accuracy- and F1-scores.\n",
    "acc_val = np.empty([num_folds,num_classifiers])\n",
    "f1_val = np.empty([num_folds, num_classifiers])\n",
    "roc_auc_val = np.empty([num_folds, num_classifiers])\n",
    "specificity_val = np.empty([num_folds, num_classifiers])\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "specificity_list = []\n",
    "sensitivity_list = []\n",
    "\n",
    "# Splits up our data into training and validation sets at a 80/20 ratio. The group_kfold does training across folds,\n",
    "# with a default of 5 folds it will give us 5 outputs.\n",
    "for i, (train_index, val_index) in enumerate(group_kfold.split(x_train_data, y_train_data, patient_id_train_data)):\n",
    "    \n",
    "    # x_train = 80%\n",
    "    # y_train = truth for 80%\n",
    "    # x_val = 20%\n",
    "    # y_val = truth for 20%\n",
    "    x_train = x[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    x_val = x[val_index,:]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    " \n",
    "    # Standardize features\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    \n",
    "    \n",
    "    acc_l = []\n",
    "    f1_l = []\n",
    "    roc_auc_l = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    \n",
    "    for j, clf in enumerate(classifiers): \n",
    "        # Train the classifier with the 80%.\n",
    "        clf.fit(x_train_scaled, y_train)\n",
    "        \n",
    "        # Predict labels for validation data\n",
    "        y_pred = clf.predict(x_val_scaled)\n",
    "    \n",
    "        # Evaluate accuracy score (mostly useless)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        acc_val[i, j] = accuracy\n",
    "        acc_l.append(accuracy)\n",
    "        \n",
    "        # Evaluate F1 score (Shows us the ratio of false positives and true negatives)\n",
    "        multi_cm = multilabel_confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        f1_val[i, j] = f1\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        \n",
    "        # Calculate ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict_proba(x_val_scaled), multi_class='ovr')\n",
    "        roc_auc_val[i, j] = roc_auc\n",
    "        roc_auc_l.append(roc_auc)\n",
    "        \n",
    "        # Calculate specificity\n",
    "        c1, c2, c3, c4 = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        tn = c1[0] # true negative\n",
    "        tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "        fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "        fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "        specificity.append(tn/(tn+fp))\n",
    "        \n",
    "        sensitivity.append(tp/(tp+fn))\n",
    "        \n",
    "        \n",
    "        # Print our scores - Classifier 1 is K-NN(1) and Classifier 2 is K-NN(5):\n",
    "        #print(f\"F1 score (Fold {i + 1}, Classifier {j + 1}): {f1_l}\")\n",
    "        #print(f\"ROC AUC score (Fold {i + 1}, Classifier {j + 1}): {roc_auc_l}\")\n",
    "        #print(f\"Specificity (Fold {i + 1}, Classifier {j + 1}): {specificity}\")\n",
    "        #print(f\"Sensitivity (Fold {i + 1}, Classifier {j + 1}): {sensitivity}\")\n",
    "        #print(\"\")\n",
    "    acc_list.append(np.mean(acc_l))\n",
    "    f1_list.append(np.mean(f1_l))\n",
    "    roc_auc_list.append(np.mean(roc_auc_l))\n",
    "    specificity_list.append(np.mean(specificity))\n",
    "    sensitivity_list.append(np.mean(sensitivity))\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a0613063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNeighborsClassifier(n_neighbors=1) is: 0.46820809248554907\n",
      "F1 for KNeighborsClassifier(n_neighbors=1) is: 0.4316032775310559\n",
      "ROC-AUC for KNeighborsClassifier(n_neighbors=1) is: 0.5158829043106369\n",
      "Specificity for KNeighborsClassifier(n_neighbors=1) is: 0.6477251163623343\n",
      "Sensitivity for KNeighborsClassifier(n_neighbors=1) is: 0.23671839271839273\n",
      "\n",
      "Accuracy for KNeighborsClassifier(n_neighbors=3) is: 0.48554913294797686\n",
      "F1 for KNeighborsClassifier(n_neighbors=3) is: 0.4553187675840925\n",
      "ROC-AUC for KNeighborsClassifier(n_neighbors=3) is: 0.5285584404618169\n",
      "Specificity for KNeighborsClassifier(n_neighbors=3) is: 0.5823306141188477\n",
      "Sensitivity for KNeighborsClassifier(n_neighbors=3) is: 0.363392829900839\n",
      "\n",
      "Accuracy for KNeighborsClassifier() is: 0.48670520231213876\n",
      "F1 for KNeighborsClassifier() is: 0.4474540279612591\n",
      "ROC-AUC for KNeighborsClassifier() is: 0.5547993418315363\n",
      "Specificity for KNeighborsClassifier() is: 0.6514256426739407\n",
      "Sensitivity for KNeighborsClassifier() is: 0.27558664593974386\n",
      "\n",
      "Accuracy for KNeighborsClassifier(n_neighbors=7) is: 0.4601156069364162\n",
      "F1 for KNeighborsClassifier(n_neighbors=7) is: 0.4197958299793057\n",
      "ROC-AUC for KNeighborsClassifier(n_neighbors=7) is: 0.5158990434349636\n",
      "Specificity for KNeighborsClassifier(n_neighbors=7) is: 0.64123132365254\n",
      "Sensitivity for KNeighborsClassifier(n_neighbors=7) is: 0.24252894082008006\n",
      "\n",
      "Accuracy for KNeighborsClassifier(n_neighbors=9) is: 0.48604651162790696\n",
      "F1 for KNeighborsClassifier(n_neighbors=9) is: 0.44520319438026856\n",
      "ROC-AUC for KNeighborsClassifier(n_neighbors=9) is: 0.6224628382940468\n",
      "Specificity for KNeighborsClassifier(n_neighbors=9) is: 0.6798935618001222\n",
      "Sensitivity for KNeighborsClassifier(n_neighbors=9) is: 0.29808406843090085\n",
      "\n",
      "Standard deviation for accuracy is: 0.011054144339940385\n",
      "Standard deviation for F1 is: 0.012616938921182303\n",
      "Standard deviation for ROC-AUC is: 0.04007430420010966\n",
      "Standard deviation for specificity is: 0.03196133512694603\n",
      "Standard deviation for sensitivity is: 0.0458803752423692\n"
     ]
    }
   ],
   "source": [
    "# Print the mean values for each classifier across the folds:\n",
    "for i in range(len(classifiers)):\n",
    "    print(f\"Accuracy for {classifiers[i]} is: {acc_list[i]}\")\n",
    "    print(f\"F1 for {classifiers[i]} is: {f1_list[i]}\")\n",
    "    print(f\"ROC-AUC for {classifiers[i]} is: {roc_auc_list[i]}\")\n",
    "    print(f\"Specificity for {classifiers[i]} is: {specificity_list[i]}\")\n",
    "    print(f\"Sensitivity for {classifiers[i]} is: {sensitivity_list[i]}\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "print(f\"Standard deviation for accuracy is: {np.std(acc_list)}\")\n",
    "print(f\"Standard deviation for F1 is: {np.std(f1_list)}\")\n",
    "print(f\"Standard deviation for ROC-AUC is: {np.std(roc_auc_list)}\")\n",
    "print(f\"Standard deviation for specificity is: {np.std(specificity_list)}\")\n",
    "print(f\"Standard deviation for sensitivity is: {np.std(sensitivity_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c92b981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define classifers:\n",
    "\n",
    "#train_score = {}\n",
    "#test_score = {}\n",
    "classifier = KNeighborsClassifier(5)\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_data)\n",
    "\n",
    "# Transform both the training and testing data using the fitted scaler\n",
    "x_train_data_scaled = scaler.transform(x_train_data)\n",
    "x_test_data_scaled = scaler.transform(x_test_data)\n",
    "\n",
    "# Fit the classifier on the scaled training data\n",
    "classifier.fit(x_train_data_scaled, y_train_data)\n",
    "\n",
    "\n",
    "y_train_pred = classifier.predict(x_train_data_scaled)\n",
    "y_test_pred = classifier.predict(x_test_data_scaled)\n",
    "\n",
    "# Calculate training and testing scores\n",
    "train_score = classifier.score(x_train_data_scaled, y_train_data)\n",
    "test_score = classifier.score(x_test_data_scaled, y_test_data)\n",
    "\n",
    "\n",
    "# Evaluation scores, from earlier!\n",
    "train_f1_score = f1_score(y_train_data, y_train_pred, average='weighted')\n",
    "test_f1_score = f1_score(y_test_data, y_test_pred, average=\"weighted\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and a category for none of them.)\n",
    "        \n",
    "# Calculate ROC AUC score\n",
    "train_roc_auc = roc_auc_score(y_train_data, clf.predict_proba(x_train_data_scaled), multi_class='ovr')\n",
    "test_roc_auc = roc_auc_score(y_test_data, clf.predict_proba(x_test_data_scaled), multi_class='ovr')\n",
    "        \n",
    "# Calculate specificity (first for train)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_train_data, y_train_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "train_specificity = (tn/(tn+fp))\n",
    "        \n",
    "train_sensitivity = (tp/(tp+fn))\n",
    "\n",
    "# Calculate specificity (now for test)\n",
    "c1, c2, c3, c4 = confusion_matrix(y_test_data, y_test_pred)\n",
    "        \n",
    "tn = c1[0] # true negative\n",
    "tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "test_specificity = (tn/(tn+fp))\n",
    "        \n",
    "test_sensitivity = (tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4261e3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for train data: 0.6435185185185185\n",
      "Accuracy score for validation data: 0.5299539170506913\n",
      "Train F1 score: 0.5998769825708896\n",
      "Test F1 score: 0.4873802616313825\n",
      "Train ROC-AUC score: 0.704167483569574\n",
      "Test ROC-AUC score: 0.6758400477007394\n",
      "Train specificity: 0.8123667377398721\n",
      "Test specificity: 0.7155172413793104\n",
      "Train sensitivity: 0.4430379746835443\n",
      "Test sensitivity: 0.31683168316831684\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score for train data: {train_score}\")\n",
    "print(f\"Accuracy score for validation data: {test_score}\")\n",
    "print(f\"Train F1 score: {train_f1_score}\")\n",
    "print(f\"Test F1 score: {test_f1_score}\")\n",
    "print(f\"Train ROC-AUC score: {train_roc_auc}\")\n",
    "print(f\"Test ROC-AUC score: {test_roc_auc}\")\n",
    "print(f\"Train specificity: {train_specificity}\")\n",
    "print(f\"Test specificity: {test_specificity}\")\n",
    "print(f\"Train sensitivity: {train_sensitivity}\")\n",
    "print(f\"Test sensitivity: {test_sensitivity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "467e66d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train our model on all of x and y now that we have validated our results.\n",
    "classifier = classifier.fit(x,y)\n",
    "\n",
    "#This is the classifier you need to save using pickle, add this to your zip file submission\n",
    "filename = 'groupXY_classifier.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
