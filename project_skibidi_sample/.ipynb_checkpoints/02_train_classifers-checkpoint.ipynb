{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a6b60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Default packages for the minimum example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import pickle #for saving/loading trained classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a8d7b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding our files - make sure to follow the same file/folder structure.\n",
    "file_data = '.' + os.sep + 'data' + os.sep +'metadata.csv'\n",
    "path_image = '.' + os.sep + 'data' + os.sep + 'images' + os.sep + 'imgs_part_1'\n",
    "\n",
    "# read the metadata csv and find the diagnostic labels.\n",
    "df = pd.read_csv(file_data)\n",
    "\n",
    "# Find the features from the feature extraction.\n",
    "file_features = 'features/features.csv'\n",
    "feature_names = ['file_name','asymmetry','color','blue-white_veil']\n",
    "\n",
    "# Load up the features in a separate dataframe to filter our metadata.\n",
    "df_features = pd.read_csv(file_features)\n",
    "\n",
    "# our_list for all images in the metadata that we also have in our features.csv\n",
    "our_list = list(np.array(df_features[\"file_name\"]))\n",
    "filtered_data = df[df[\"img_id\"].isin(our_list)]\n",
    "label = np.array(filtered_data['diagnostic'])\n",
    "image_id = list(filtered_data['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1607c361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# IGNORE - old Y\\ny =  np.int_(filtered_label == 'MEL', 'BCC', 'SCC')   #now True means healthy nevus, False means something else\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the dataset\n",
    "x = np.array(df_features[feature_names[1:]])\n",
    "\n",
    "y = np.zeros(len(label))  # Initialize the labels array with zeros\n",
    "y[label == 'BCC'] = 1     # Set BCC samples to 1\n",
    "y[label == 'SCC'] = 2     # Set SCC samples to 2\n",
    "y[label == 'MEL'] = 3     # Set MEL samples to 3\n",
    "patient_id = filtered_data['patient_id']\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# IGNORE - old Y\n",
    "y =  np.int_(filtered_label == 'MEL', 'BCC', 'SCC')   #now True means healthy nevus, False means something else\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42cde72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       PAT_1516\n",
       "1         PAT_46\n",
       "2       PAT_1545\n",
       "3       PAT_1989\n",
       "4        PAT_684\n",
       "          ...   \n",
       "1178     PAT_931\n",
       "1179     PAT_678\n",
       "1180     PAT_810\n",
       "1181      PAT_83\n",
       "1182    PAT_1220\n",
       "Name: patient_id, Length: 1081, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, x_test_data, y_train_data, y_test_data, patient_id_train_data, patient_id_test_data = train_test_split(\n",
    "    x, y, patient_id, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb22210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare cross-validation -\n",
    "# GroupKFold makes sure patients with the same ID will not be split between the training and validation sets.\n",
    "\n",
    "num_folds = 5\n",
    "group_kfold = GroupKFold(n_splits=num_folds)\n",
    "group_kfold.get_n_splits(x_train_data, y_train_data, patient_id_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "991a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classifiers are defined here. We use K-NN\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(1),\n",
    "    KNeighborsClassifier(5)\n",
    "]\n",
    "num_classifiers = len(classifiers)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d227ae31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (Fold 1, Classifier 1): 0.2771048698468053\n",
      "ROC AUC score (Fold 1, Classifier 1): 0.5225883837874823\n",
      "Specificity (Fold 1, Classifier 1): [0.5]\n",
      "Sensitivity (Fold 1, Classifier 1): [0.3968253968253968]\n",
      "F1 score (Fold 1, Classifier 2): 0.22566722268557132\n",
      "ROC AUC score (Fold 1, Classifier 2): 0.532763842478447\n",
      "Specificity (Fold 1, Classifier 2): [0.5, 0.6979166666666666]\n",
      "Sensitivity (Fold 1, Classifier 2): [0.3968253968253968, 0.2077922077922078]\n",
      "F1 score (Fold 2, Classifier 1): 0.29800591595267234\n",
      "ROC AUC score (Fold 2, Classifier 1): 0.5396652044781591\n",
      "Specificity (Fold 2, Classifier 1): [0.46153846153846156]\n",
      "Sensitivity (Fold 2, Classifier 1): [0.4927536231884058]\n",
      "F1 score (Fold 2, Classifier 2): 0.24898989898989898\n",
      "ROC AUC score (Fold 2, Classifier 2): 0.5272483269614651\n",
      "Specificity (Fold 2, Classifier 2): [0.46153846153846156, 0.6082474226804123]\n",
      "Sensitivity (Fold 2, Classifier 2): [0.4927536231884058, 0.32894736842105265]\n",
      "F1 score (Fold 3, Classifier 1): 0.3090828440147053\n",
      "ROC AUC score (Fold 3, Classifier 1): 0.5169876715798495\n",
      "Specificity (Fold 3, Classifier 1): [0.48514851485148514]\n",
      "Sensitivity (Fold 3, Classifier 1): [0.2916666666666667]\n",
      "F1 score (Fold 3, Classifier 2): 0.2668153102935712\n",
      "ROC AUC score (Fold 3, Classifier 2): 0.5755935567295422\n",
      "Specificity (Fold 3, Classifier 2): [0.48514851485148514, 0.7010309278350515]\n",
      "Sensitivity (Fold 3, Classifier 2): [0.2916666666666667, 0.3157894736842105]\n",
      "F1 score (Fold 4, Classifier 1): 0.2745295394584992\n",
      "ROC AUC score (Fold 4, Classifier 1): 0.5269944445825477\n",
      "Specificity (Fold 4, Classifier 1): [0.5148514851485149]\n",
      "Sensitivity (Fold 4, Classifier 1): [0.4027777777777778]\n",
      "F1 score (Fold 4, Classifier 2): 0.2342775041050903\n",
      "ROC AUC score (Fold 4, Classifier 2): 0.5183943274094756\n",
      "Specificity (Fold 4, Classifier 2): [0.5148514851485149, 0.6808510638297872]\n",
      "Sensitivity (Fold 4, Classifier 2): [0.4027777777777778, 0.24050632911392406]\n",
      "F1 score (Fold 5, Classifier 1): 0.24726890756302522\n",
      "ROC AUC score (Fold 5, Classifier 1): 0.5082470686255951\n",
      "Specificity (Fold 5, Classifier 1): [0.4782608695652174]\n",
      "Sensitivity (Fold 5, Classifier 1): [0.4125]\n",
      "F1 score (Fold 5, Classifier 2): 0.266869918699187\n",
      "ROC AUC score (Fold 5, Classifier 2): 0.6600743975833969\n",
      "Specificity (Fold 5, Classifier 2): [0.4782608695652174, 0.7647058823529411]\n",
      "Sensitivity (Fold 5, Classifier 2): [0.4125, 0.2988505747126437]\n"
     ]
    }
   ],
   "source": [
    "# set up np arrays for the eventual accuracy- and F1-scores.\n",
    "acc_val = np.empty([num_folds,num_classifiers])\n",
    "f1_val = np.empty([num_folds, num_classifiers])\n",
    "roc_auc_val = np.empty([num_folds, num_classifiers])\n",
    "specificity_val = np.empty([num_folds, num_classifiers])\n",
    "\n",
    "f1_list = []\n",
    "# Splits up our data into training and validation sets at a 80/20 ratio. The group_kfold does training across folds,\n",
    "# with a default of 5 folds it will give us 5 outputs.\n",
    "for i, (train_index, val_index) in enumerate(group_kfold.split(x_train_data, y_train_data, patient_id_train_data)):\n",
    "    \n",
    "    # x_train = 80%\n",
    "    # y_train = truth for 80%\n",
    "    # x_val = 20%\n",
    "    # y_val = truth for 20%\n",
    "    x_train = x[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    x_val = x[val_index,:]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    " \n",
    "    # Standardize features\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    \n",
    "    \n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    \n",
    "    for j, clf in enumerate(classifiers): \n",
    "        # Train the classifier with the 80%.\n",
    "        clf.fit(x_train_scaled, y_train)\n",
    "        \n",
    "        # Predict labels for validation data\n",
    "        y_pred = clf.predict(x_val_scaled)\n",
    "    \n",
    "        # Evaluate accuracy score (mostly useless)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        acc_val[i, j] = accuracy\n",
    "        \n",
    "        # Evaluate F1 score (Shows us the ratio of false positives and true negatives)\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")  # Use macro-average for multi-class classification (We have 4 classes -> the 3 skin cancers and neither of them.)\n",
    "        f1_val[i, j] = f1\n",
    "        \n",
    "        # Calculate ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict_proba(x_val_scaled), multi_class='ovr')\n",
    "        roc_auc_val[i, j] = roc_auc\n",
    "        \n",
    "        # Calculate specificity\n",
    "        c1, c2, c3, c4 = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        tn = c1[0] # true negative\n",
    "        tp = c2[1] + c3[2] + c4[3] # true positive\n",
    "        fp = np.sum(c1[1:]) + np.sum(c2[2:]) + c3[3] + c3[1] + np.sum(c4[1:2]) # false positive\n",
    "        fn = c2[0] + c3[0] + c4[0] # false negative\n",
    "        \n",
    "        specificity.append(tn/(tn+fp))\n",
    "        specificity_avg = np.mean(specificity)\n",
    "        \n",
    "        sensitivity.append(tp/(tp+fn))\n",
    "        sensitivity_avg = np.mean(sensitivity)\n",
    "        \n",
    "        \n",
    "        # Print our scores - Classifier 1 is K-NN(1) and Classifier 2 is K-NN(5):\n",
    "        print(f\"F1 score (Fold {i + 1}, Classifier {j + 1}): {f1}\")\n",
    "        print(f\"ROC AUC score (Fold {i + 1}, Classifier {j + 1}): {roc_auc}\")\n",
    "        print(f\"Specificity (Fold {i + 1}, Classifier {j + 1}): {specificity}\")\n",
    "        print(f\"Sensitivity (Fold {i + 1}, Classifier {j + 1}): {sensitivity}\")  \n",
    "        \n",
    "        \n",
    "        f1_list.append(f1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ff07aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65 14  0  0]\n",
      " [48 26  0  0]\n",
      " [12  6  0  0]\n",
      " [ 1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6cca8ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1[0] # true negative\n",
    "\n",
    "c2[1] + c3[2] + c4[3] # true positive\n",
    "\n",
    "c0[1:] + c1[2:] + c2[3] + c2[1] + c3[1:2] # false positive\n",
    "\n",
    "c1[0] + c2[0] + c3[0] # false negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8800912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 1 average accuracy=0.451 \n",
      "Classifier 2 average accuracy=0.501 \n"
     ]
    }
   ],
   "source": [
    "#Average over all folds\n",
    "average_acc = np.mean(acc_val,axis=0) \n",
    "   \n",
    "print('Classifier 1 average accuracy={:.3f} '.format(average_acc[0]))\n",
    "print('Classifier 2 average accuracy={:.3f} '.format(average_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PART - To be expanded on. Basically this is the data we will want to compare to the external pictures\n",
    "# If we want to include all the pictures here, we no longer have any \"true\" set to compare our results to. \n",
    "# We should consider creating a 3-way split, so we have a final, unused picture set to work on our model.\n",
    "\n",
    "# Let's say you now decided to use the 5-NN \n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "#It will be tested on external data, so we can try to maximize the use of our available data by training on \n",
    "#ALL of x and y\n",
    "classifier = classifier.fit(x,y)\n",
    "\n",
    "#This is the classifier you need to save using pickle, add this to your zip file submission\n",
    "filename = 'groupXY_classifier.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
